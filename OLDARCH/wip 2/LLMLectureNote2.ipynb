{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f4d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "transformerarch=Image(url= \"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*THCl9vdQot0HNZRp6YmzMw.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab0194",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PreRequisite\n",
    "Temperature-checking - how many of these concepts are we not familiar with?\n",
    "1. Introduction to Machine Learning: ML, Supervised/Unsupervised;\n",
    "2. Deep Learning (Basics): Neural networks, activation functions, backpropagation, gradient desent;\n",
    "3. Convolutional Neural Networks: CNN & application in CV tasks;\n",
    "4. REcurrent Neural Networks (RNNs): Basics, LSTM and GRU & usage in sequence data;\n",
    "5. Intro to NLP: Overview of NLP - tokenization, word embeddings, sentiment analysis;\n",
    "6. Seq2Seq models: how they work and their role in tasks like machine translation;\n",
    "7. Attention Mechanism: Attention, Significance in transforming sequences;\n",
    "8. Transformers and BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e1b52",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lecture Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad9cdc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. **Introduction to LLMs**: What LLMs are and why they're important (W1)\n",
    "\n",
    "2. **Training LLMs**: On LLM's training including challenges and techniques involved (W1)\n",
    "\n",
    "3. **Understanding GPT Architecture**: Detailed study of the GPT architecture used in LLMs. (W1)\n",
    "\n",
    "4. **Fine-tuning Large Language Models**: Understand the concept of fine-tuning and its application in LLMs. (W2)\n",
    "\n",
    "5. **Applications of LLMs**: Explore real-world applications of LLMs in various industries. (W2)\n",
    "\n",
    "6. **Evaluating LLMs**: Learn about different metrics and methods to evaluate the performance of LLMs. (W3)\n",
    "\n",
    "7. **Bias in LLMs**: Discuss the potential for bias in LLMs and how to mitigate it. (W3)\n",
    "\n",
    "8. **Limitations and Future of LLMs**: Discuss the current limitations of LLMs and future research directions. (W4)\n",
    "\n",
    "9. **Case Study - GPT-3**: A detailed case study on GPT-3, its architecture, training, and applications. (W4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838564d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introduction to LLMs and how they are trained\n",
    "A type of ML model designed to understand, generate and converse in human language, 'large' due to the vast number of parameters.\n",
    "- Ability to generate human-like texts\n",
    "- Patterns in data used to train the model learnt allows the models to generate text based on received inputs\n",
    "- Rule of thumb: 7B parameters takes one sota GPU to run, i.e. 13B takes two, etc.\n",
    "- LLMs can perform natural language processing (NLP) tasks, note LLM â‰  NLP model\n",
    "- OpenAI first released GPT-1 in 2018, and GPT 3 in 2020, where terrabytes of data were used to train these models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f57577",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Architecture of LLM \n",
    "'Attention is All you Need', seminal paper on the most commonly used architecture known as **transformer** was first proposed, revoluntionized the field of NLP.\n",
    "- Transformers are based on the **attention** mechanism, which allows the model to better associate words w.r.t. their positions, of primarily two types:\n",
    "    - self-attention\n",
    "    - multi-head attention\n",
    "- Transformer = **encoder** (input) + **decoder** (Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac99ef3b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*THCl9vdQot0HNZRp6YmzMw.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformerarch\n",
    "#For those more keen on looking at the two components - note that GPT-3 is decoder only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9c1bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Architecture of LLM - Transformer Layer Types\n",
    "1. Self-Attention Layer: Scaled Dot-Product Attention\n",
    "    - Allows model to focus on different parts of the input sequence when producing an output.\n",
    "        - e.g. \"The cat, which already ate...., was full.\" can prioritize \"The cat\" with \"was full\" before \"already ate\";\n",
    "2. Position-wise Feed-Forward Layer\n",
    "    - Fully-connected feed-forward network that is applied to each position separately and identically, each consists two linear transformations with a ReLU activation in between;\n",
    "    - Used to sequentially process the output of the self-attention layer, appying the same learnt weights at every position;\n",
    "3. Output layer\n",
    "    - Linear layer followed by a softmax function, transforms the final hidden states to predictions for the next word in the sequence for each possible word in the vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa492c38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Architecture of LLM -  Data Handling by GPT Models\n",
    "\n",
    "#### Tokenization\n",
    "Process of converting sequence of text into a sequence of tokens - a (part of) word. _Byte Pair Encoding_\n",
    "platform.openai.com/tokenizer\n",
    "\n",
    "#### BPE\n",
    "- subwod tokenization method, \n",
    "    - starts by splitting text into individual characters and \n",
    "    - iteratively merges the most frequently adjacent pair of symbols. \n",
    "- helps handle out-of-vocabulary words and makes the model more robust to spelling errors and variations\n",
    "\n",
    "#### Sequence length\n",
    "- context window size (2048 for GPT-3, 8K for GPT-4 or 32K)\n",
    "- Limit on the number of tokens they can be passed at once due to memory constraints\n",
    "- For an decoder-only model takes the input sequence all at once, where each token and its relationships with every other token in the sequence in memory\n",
    "- quadratic increase in memory usage at the sequence length grows\n",
    "- Primarily to make the model feasible to run on available hardware\n",
    "- **Longer (input + output) sequence takes very long to generate**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860059d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Architecture of LLM -  Implementation and deployment of GPT\n",
    "\n",
    "#### Implementation\n",
    "- Unlikely going to have retraining schedules.\n",
    "- May require initial fine-tuning or tweaking OR \n",
    "- Vanilla\n",
    "- GPT-in-general: Implemented with Tensorflow or PyTorch: efficient, GPU-accelerated operations\n",
    "\n",
    "#### Deployment Process\n",
    "- Models needs to reside on robust hardware or cloud-based solutions: consider numerous simultaneous requests\n",
    "- Typically involves API-calls made to request services from models deployed on network\n",
    "- Requests (e.g. text generation, text-completion) are handled and responses sent back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6cf74e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Architecture of LLM -  Back to the SOTA models\n",
    "GPT-3, GPT-J, etc.: An **autoregressive, decoder-only transformer model** designed to solve natural language processing (NLP) tasks by **predicting** how a piece of text will continue.\n",
    "\n",
    "This is different from traditional encoder-decoder transformer models like BERT where the inputs are first encoded and thrown together at the model as a whole when making the prediction.\n",
    "\n",
    "GPT-3-like decoder-only models puts more emphasis on the more recent inputs, maknig the prediction continuously being more relevant with the more recent information. \n",
    "\n",
    "- Advantage of decoder-only architecture:\n",
    "    - Simplicity: easier to train less computationally expensive\n",
    "    - Good at generative tasks, producing contextually relevant text - model is built to generate output one token at a time;\n",
    "- Advantage of encoder-decoder model:\n",
    "    - Better at classification tasks - tasks where specific structure is needed, e.g. translation, summarization, particularly good when input information comes all at once\n",
    "\n",
    "Note here these are just high-level benefits, YMMV with different use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab0bcd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A few notes:\n",
    "\n",
    "1. _Autoregressiveness equals no parallelized during inference_\n",
    "2. _Autoencoder_ models should be differentiated from encoder-decoder model: \n",
    "    - the prior also has an encoder and a decoder but primarily serves the purpose of dimension-reduction and denoising, aka 'learning' the input while\n",
    "    - the latter is designed to work in sequence-to-sequence tasks.\n",
    "    - i.e. autoencoder's output is representation of its input\n",
    "    - encoder-decoder is not just a reconstruction of the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66ccad5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tying this back to GPT-3 models (and its alikes): LLMs performs sequence-to-sequence tasks but puts more weights to more recent context, yet it still considers all previous tokens (history) to predict the upcoming token. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a322fa10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Architecture of LLM -  Generative Pre-trained Transformer aka OpenAI's proposal\n",
    "\n",
    "1. GPT-1: Improving Language Understanding by Generative Pre-training\" in June 2018. 12 transformer layers and 117m parameters;\n",
    "2. GPT-2: Released in 2019, has 1.5B parameters, text can become indistinguiashable to text written by humans;\n",
    "3. GPT-3: Launched in 2020, (disclosed) largeset variant at 175B parameters, generate coherent and contextually relevant passages of text, introduction of 'few-shot learning'.\n",
    "    - translation\n",
    "    - Q&A\n",
    "    - Writing creative contents like poems and stories\n",
    "4. GPT-4: Released in 2023, no. of parameter undisclosed, still known as the most powerful LLM that has been released;\n",
    "\n",
    "\n",
    "GPT-3.5: sub-class of GPT-3 first released in Apr. 2022, leading to '**chatGPT**' in Nov. 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7399dd3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
