{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f4d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "transformerarch=Image(url= \"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab0194",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PreRequisite\n",
    "Temperature-checking - how many of these concepts are we not familiar with?\n",
    "1. Introduction to Machine Learning: ML, Supervised/Unsupervised;\n",
    "2. Deep Learning (Basics): Neural networks, activation functions, backpropagation, gradient desent;\n",
    "3. Convolutional Neural Networks: CNN & application in CV tasks;\n",
    "4. Recurrent Neural Networks (RNNs): Basics, LSTM and GRU & usage in sequence data;\n",
    "5. Intro to NLP: Overview of NLP - tokenization, word embeddings, sentiment analysis;\n",
    "6. Seq2Seq models: how they work and their role in tasks like machine translation;\n",
    "7. Attention Mechanism: Attention, Significance in transforming sequences;\n",
    "8. Transformers and BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e1b52",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lecture Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad9cdc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. **Introduction to LLMs**: What LLMs are and why they're important\n",
    "\n",
    "2. **Training LLMs**: On LLM's training including challenges and techniques involved \n",
    "\n",
    "3. **Understanding GPT Architecture**: Understand GPT architecture used in LLMs.\n",
    "\n",
    "4. **Fine-tuning Large Language Models**: Concept of fine-tuning and its application in LLMs. \n",
    "\n",
    "5. **Applications of LLMs**: Examine real-world applications of LLMs within various use cases. \n",
    "\n",
    "6. **Evaluating LLMs**: Learn about different metrics and methods and their _caveats_ to evaluate the performance of LLMs.\n",
    "\n",
    "7. **Bias in LLMs**: Discuss the potential for bias in LLMs and how to mitigate it.\n",
    "\n",
    "8. **Limitations and Future of LLMs**: Discuss the current limitations of LLMs and future research directions.\n",
    "\n",
    "9. **Hackathon-related refreshers**: Stochastics of time seires, API-based call of LLMs and Graphical Programming Interface that requires little-to-zero coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838564d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introduction to LLMs and how they are trained\n",
    "A type of ML model designed to understand, generate and converse in human language, 'large' due to the vast number of parameters.\n",
    "- Ability to generate human-like texts\n",
    "- Patterns in data used to train the model learnt allows the models to generate text based on received inputs\n",
    "- Rule of thumb: 7B parameters takes one sota GPU to run, i.e. 13B takes two, etc.\n",
    "- LLMs can perform natural language processing (NLP) tasks, note LLM â‰  NLP model\n",
    "- OpenAI first released GPT-1 in 2018, and GPT 3 in 2020, where terrabytes of data were used to train these models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44b2309",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's see some LLMs in action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f57577",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Architecture of LLM \n",
    "'Attention is All you Need', seminal paper on the most commonly used architecture known as **transformer** was first proposed, revoluntionized the field of NLP.\n",
    "- Transformers are based on the **attention** mechanism, which allows the model to better associate words w.r.t. their positions, of primarily two types:\n",
    "    - self-attention\n",
    "    - multi-head attention\n",
    "- Transformer = **encoder** (input) + **decoder** (Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac99ef3b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformerarch\n",
    "#For those more keen on looking at the two components - note that GPT-3 is decoder only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9c1bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Architecture of LLM - Transformer Layer Types\n",
    "1. Self-Attention Layer: Scaled Dot-Product Attention\n",
    "    - Allows model to focus on different parts of the input sequence when producing an output.\n",
    "        - e.g. \"The cat, which already ate...., was full.\" can prioritize \"The cat\" with \"was full\" before \"already ate\";\n",
    "2. Position-wise Feed-Forward Layer\n",
    "    - Fully-connected feed-forward network that is applied to each position separately and identically, each consists two linear transformations with a ReLU activation in between;\n",
    "    - Used to sequentially process the output of the self-attention layer, appying the same learnt weights at every position;\n",
    "3. Output layer\n",
    "    - Linear layer followed by a softmax function, transforms the final hidden states to predictions for the next word in the sequence for each possible word in the vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa492c38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Architecture of LLM -  Data Handling by GPT Models\n",
    "\n",
    "#### Tokenization\n",
    "Process of converting sequence of text into a sequence of tokens - a (part of) word. _Byte Pair Encoding_\n",
    "platform.openai.com/tokenizer\n",
    "\n",
    "#### BPE\n",
    "- subwod tokenization method, \n",
    "    - starts by splitting text into individual characters and \n",
    "    - iteratively merges the most frequently adjacent pair of symbols. \n",
    "- helps handle out-of-vocabulary words and makes the model more robust to spelling errors and variations\n",
    "\n",
    "#### Sequence length\n",
    "- context window size (2048 for GPT-3, 8K for GPT-4 or 32K)\n",
    "- Limit on the number of tokens they can be passed at once due to memory constraints\n",
    "- For an decoder-only model takes the input sequence all at once, where each token and its relationships with every other token in the sequence in memory\n",
    "- quadratic increase in memory usage at the sequence length grows\n",
    "- Primarily to make the model feasible to run on available hardware\n",
    "- **Longer (input + output) sequence takes very long to generate**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860059d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Architecture of LLM -  Implementation and deployment of GPT\n",
    "\n",
    "#### Implementation\n",
    "- Unlikely going to have retraining schedules.\n",
    "- May require initial fine-tuning or tweaking OR \n",
    "- Vanilla\n",
    "- GPT-in-general: Implemented with Tensorflow or PyTorch: efficient, GPU-accelerated operations\n",
    "\n",
    "#### Deployment Process\n",
    "- Models needs to reside on robust hardware or cloud-based solutions: consider numerous simultaneous requests\n",
    "- Typically involves API-calls made to request services from models deployed on network\n",
    "- Requests (e.g. text generation, text-completion) are handled and responses sent back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6cf74e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Architecture of LLM -  Back to the SOTA models\n",
    "GPT-3, GPT-J, etc.: An **autoregressive, decoder-only transformer model** designed to solve natural language processing (NLP) tasks by **predicting** how a piece of text will continue.\n",
    "\n",
    "This is different from traditional encoder-decoder transformer models like BERT where the inputs are first encoded and thrown together at the model as a whole when making the prediction.\n",
    "\n",
    "GPT-3-like decoder-only models puts more emphasis on the more recent inputs, maknig the prediction continuously being more relevant with the more recent information. \n",
    "\n",
    "- Advantage of decoder-only architecture:\n",
    "    - Simplicity: easier to train less computationally expensive\n",
    "    - Good at generative tasks, producing contextually relevant text - model is built to generate output one token at a time;\n",
    "- Advantage of encoder-decoder model:\n",
    "    - Better at classification tasks - tasks where specific structure is needed, e.g. translation, summarization, particularly good when input information comes all at once\n",
    "\n",
    "Note here these are just high-level benefits, YMMV with different use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab0bcd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A few notes:\n",
    "\n",
    "1. _Autoregressiveness equals no parallelized during inference_\n",
    "2. _Autoencoder_ models should be differentiated from encoder-decoder model: \n",
    "    - the prior also has an encoder and a decoder but primarily serves the purpose of dimension-reduction and denoising, aka 'learning' the input while\n",
    "    - the latter is designed to work in sequence-to-sequence tasks.\n",
    "    - i.e. autoencoder's output is representation of its input\n",
    "    - encoder-decoder is not just a reconstruction of the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66ccad5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tying this back to GPT-3 models (and its alikes): LLMs performs sequence-to-sequence tasks but puts more weights to more recent context, yet it still considers all previous tokens (history) to predict the upcoming token. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a322fa10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Architecture of LLM -  Generative Pre-trained Transformer aka OpenAI's proposal\n",
    "\n",
    "1. GPT-1: Improving Language Understanding by Generative Pre-training\" in June 2018. 12 transformer layers and 117m parameters;\n",
    "2. GPT-2: Released in 2019, has 1.5B parameters, text can become indistinguiashable to text written by humans;\n",
    "3. GPT-3: Launched in 2020, (disclosed) largeset variant at 175B parameters, generate coherent and contextually relevant passages of text, introduction of 'few-shot learning'.\n",
    "    - translation\n",
    "    - Q&A\n",
    "    - Writing creative contents like poems and stories\n",
    "4. GPT-4: Released in 2023, no. of parameter undisclosed, still known as the most powerful LLM that has been released;\n",
    "\n",
    "\n",
    "GPT-3.5: sub-class of GPT-3 first released in Apr. 2022, leading to '**chatGPT**' in Nov. 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bc9f23",
   "metadata": {},
   "source": [
    "### Applications of LLMs: Examine real-world applications of LLMs within various use cases.\n",
    "\n",
    "\n",
    "- Chatbots: User intention and logic. Short back-and-forth limits the need for larger context window\n",
    "- Script-writing: Hollywood on strike for a reason.\n",
    "\n",
    "Evaluating LLMs: Learn about different metrics and methods and their caveats to evaluate the performance of LLMs.\n",
    "Bias in LLMs: Discuss the potential for bias in LLMs and how to mitigate it.\n",
    "Limitations and Future of LLMs: Discuss the current limitations of LLMs and future research directions.\n",
    "Hackathon-related refreshers: Stochastics of time seires, API-based call of LLMs and Graphical Programming Interface that requires little-to-zero coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50428d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Common Selection Strategies seen in LLM Application Development\n",
    "Inference-time paramters that affect how tokens are generated by a LLM.\n",
    "- Temperature: Deterministic to Creativity\n",
    "    - Randomness of model output (0-1)\n",
    "    - When set to 0, outputs is completely reproducible\n",
    "    - When set to 1, model is more random, aka has more 'creative' outputs\n",
    "- Top_K Sampling: Number of top tokens considered for best next word\n",
    "    - limits us to a certain number of the top tokens to consider\n",
    "    \n",
    "- Top_P Sampling: Probability threshold\n",
    "    - Recall the models are statistical in nature, use a threashold of probability, e.g. 90%\n",
    "\n",
    "- Token length & Max tokens: Number of tokens fed to/generated by LLMs\n",
    "    - Relevant due to the amount of time of solution generated\n",
    "    - Also relevant to costs (Most close models charge by # of tokens used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a55b4ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12090026",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quick Recap\n",
    "- Talked about onboarding to GCP and some high-level contents w.r.t. how LLMs work\n",
    "- In particular touch upon the following concept:\n",
    "    - Temperature\n",
    "    - top_N\n",
    "    - probabilities \n",
    "- Now switching gear back to the learning series, we will talk about Application of LLMs today.\n",
    "- Let's begin with the five common pillars recognized by OpenAI (or GPT-4 itself)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16066e5a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Key Applications Recognized\n",
    "1. Content Creation\n",
    "    - write articles\n",
    "    - generate ideas/scripts for writers\n",
    "    - **creativity and fluency**\n",
    "2. Conversation Agents:\n",
    "    - chatbot/virtual assistant (Pershin X)\n",
    "    - Human-like **interaction**\n",
    "    - _Can answer queries, provide recommendations and hold conversations_\n",
    "3. Translation:\n",
    "    - not explicitly trained to do this\n",
    "    - _Can handle various languages nonetheless_\n",
    "4. Education\n",
    "    - Tutor various subjects\n",
    "    - _Can provide explanations, stimulate creative thinking_\n",
    "5. Programming Help: CoPilot\n",
    "    - _Can generate code snippets and assist with debugging_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7424ee7d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Limitations and Ethical Considerations of Large Language Models\n",
    "- Limitations\n",
    "    - Understanding vs. Pattern Matching\n",
    "    - Sensitivity to Input Phrasing\n",
    "    - Verifiability of information: generating plausible-sounding but incorrect/nonsensical information.\n",
    "        - no inherent way to verify accuracy of the information it generates\n",
    "        - __but doesn't 'grounding' solve this?__\n",
    "- Ethical Considerations\n",
    "    - Bias in Training Data\n",
    "        - gender, racial cultral biases and more\n",
    "    - Misuse of Technology\n",
    "        - deepfake text for nefarious purposes\n",
    "    - Social/Environmental Impact\n",
    "        - training of LLM is very energy intensive ad requires significant amount of computational resources\n",
    "        - Role of human/smart copiers' role in RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a55879",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How does 'Grounding' work?\n",
    "- What is 'grounding':\n",
    "    - Refers to the concept of connecting the LLM with real-world concrete data or knowledge source\n",
    "    - May include:\n",
    "        - linking to specific databases, \n",
    "        - using factual knowledge graphs, or \n",
    "        - providing the model with access to up-to-date information on the internet.\n",
    "- MSFT once claimed it will resolve most of our concerns/problems.\n",
    "    - RFP PoC/Pilot\n",
    "    - Chat with your PDF(s) demo \n",
    "- **Verfiability issue is more than just factual grounding.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f0d497",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why doesn't 'Grounding' work?\n",
    "- Grouding will improve accuracy of responses and keep them factual\n",
    "- Creates a form of question-answering system that performs semantic search over a known corpus of documents\n",
    "- Some points to consider:\n",
    "    1. Model interpretation (temp=0, aka model outputs more deterministic): model understanding â‰  human understanding;\n",
    "    2. Accuracy within context: errors acn arise from:\n",
    "        - biases/inaccuracies in training data, \n",
    "        - misinterpretation of context, \n",
    "        - limitations in the model's understanding of complex language structures;\n",
    "    3. Handling Ambiguity: question or context is ambigous, the model might struggle to provide an accurate and relevant answer.\n",
    "\n",
    "### **'Grounding' may improve verifiability of a model's response, it won't eliminate the issue.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b628248",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mitigating LLM Risks\n",
    "- Fine-tuning on specific datasets: improve behavior for **specific** applications\n",
    "    - Refers to the concept of connecting the LLM with real-world concrete data or knowledge source\n",
    "    - May include:\n",
    "        - linking to specific databases, \n",
    "        - using factual knowledge graphs, or \n",
    "        - providing the model with access to up-to-date information on the internet.\n",
    "- MSFT once claimed it will resolve most of our concerns/problems.\n",
    "    - RFP PoC/Pilot\n",
    "    - Chat with your PDF(s) demo \n",
    "- **Verfiability issue is more than just factual grounding.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dde629e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quick Re-Cap on what we discussed\n",
    "- Basics of LLMs\n",
    "    - What they are and how they are trained\n",
    "- Architecture of GPT models (Decoder-only ones started by OpenAI)\n",
    "    - Transformer architecture\n",
    "    - Decoder-only vs. encoder-decoder architectures\n",
    "- Implementation and Deployment\n",
    "    - APIs\n",
    "    - Real-world usage and applications we built\n",
    "- Limitations and Concerns\n",
    "    - Lack of human-intent-understanding capability\n",
    "    - Inability to provide verification\n",
    "    - Proneness to biases\n",
    "- Implications of LLMs\n",
    "    - Wide-ranging capabilities and applications of these models\n",
    "    - Potential risks and concerns\n",
    "    - Strategies to mitigate these risks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d051525c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Quick Re-Cap on what we discussed - Synthesized\n",
    "- Basics of LLMs\n",
    "    - Generative Pre-Trained (on large text corpora) Models \n",
    "- Architecture of GPT models (Decoder-only ones started by OpenAI)\n",
    "    - Decoder-only, no parallization \n",
    "- Implementation and Deployment\n",
    "    - OpenAI & Google: APIs\n",
    "    - Open-Source models: Google Colab, i.e. local/remote GPUs(CLI)\n",
    "- Limitations and Concerns\n",
    "    - It doesn't 'understand' prompts and human intentions\n",
    "    - But can sound very convincing as trained on great writing examples - is it a better writer?\n",
    "- Implications of LLMs\n",
    "    - Different learning experiences\n",
    "    - Revolutionize coding experience (CoPilot)\n",
    "    - Further reduction of workforce (e.g. Customer Service, call center reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02120f11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some Options to test/understand LLMs\n",
    "- Langflow\n",
    "- Flowise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bededcb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "def generate_word_cloud(text):\n",
    "    # Create a set of stopwords\n",
    "    stopwords = set(STOPWORDS)\n",
    "\n",
    "    # Generate a word cloud\n",
    "    wordcloud = WordCloud(width=2500, height=500, \n",
    "                          background_color='white', \n",
    "                          stopwords=stopwords, \n",
    "                          min_font_size=8).generate(text)\n",
    "\n",
    "    # Plot the WordCloud image\n",
    "    plt.figure(figsize=(12,6), facecolor=None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=1)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
